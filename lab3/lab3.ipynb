{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acba18f5",
   "metadata": {},
   "source": [
    "# Zadatak 1. Učitavanje podataka (25% bodova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d79080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prvi\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2565c",
   "metadata": {},
   "source": [
    "Sve metode potrebne za prvi zadatak nalaze se u datoteci prvi.py. U sljedećem isječku koda računamo frekvencije pojavljivanja riječi, stvaramo vokabulare i učitavamo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acfd624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_data, freq_label = prvi.get_frequencies('data/sst_train_raw.csv')\n",
    "text_vocab = prvi.Vocab(freq_data, max_size=-1, min_freq=1)\n",
    "label_vocab = prvi.Vocab(freq_label, use_extra=False)\n",
    "train_dataset = prvi.NLPDataset('data/sst_train_raw.csv', text_vocab, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd348ec",
   "metadata": {},
   "source": [
    "## Razred Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21cb00",
   "metadata": {},
   "source": [
    "Vaša implementacija razreda Vocab mora implementirati funkcionalnost pretvorbe niza tokena (ili jednog tokena) u brojeve. Ovu funkciju možete nazvati encode. Primjer ove pretvorbe za četvrtu instancu train skupa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4248ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['yet', 'the', 'act', 'is', 'still', 'charming', 'here']\n",
      "Label: positive\n",
      "Numericalized text: tensor([189,   2, 674,   7, 129, 348, 143])\n",
      "Numericalized label: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "instance_text, instance_label = train_dataset.instances[3]\n",
    "print(f\"Text: {instance_text}\")\n",
    "print(f\"Label: {instance_label}\")\n",
    "print(f\"Numericalized text: {text_vocab.encode(instance_text)}\")\n",
    "print(f\"Numericalized label: {label_vocab.encode(instance_label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d691360",
   "metadata": {},
   "source": [
    "Također, vaša implementacija razreda Vocab mora primati iduće parametre:\n",
    "\n",
    "max_size: maksimalni broj tokena koji se sprema u vokabular (uključuje i posebne znakove). -1 označava da se spremaju svi tokeni.\n",
    "min_freq: minimalna frekvencija koju token mora imati da bi ga se spremilo u vokabular (\\ge). Posebni znakovi ne prolaze ovu provjeru.\n",
    "Primjer izgradnje vokabulara sa svim tokenima (duljina uključuje i posebne znakove):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa5adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14806\n"
     ]
    }
   ],
   "source": [
    "text_vocab = prvi.Vocab(freq_data, max_size=-1, min_freq=0)\n",
    "print(len(text_vocab.itos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32bd474",
   "metadata": {},
   "source": [
    "## Učitavanje vektorskih reprezentacija"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4693c",
   "metadata": {},
   "source": [
    "Vaš zadatak je implementirati funkciju koja će za zadani vokabular (iterable stringova) generirati embedding matricu. Vaša funkcija treba podržavati dva načina genriranja embedding matrice: nasumična inicijalizacija iz standardne normalne razdiobe (N(0,1)) i učitavanjem iz datoteke. Pri učitavanju iz datoteke, ako ne pronađete vektorsku reprezentaciju za neku riječ, inicijalizirajte ju normalno. Vektorsku reprezentaciju za znak punjenja (na indeksu 0) morate inicijalizirati na vektor nula. Jednostavan način na koji možete implementirati ovo učitavanje je da inicijalirate matricu iz standardne normalne razdiobe, a potom prebrišete inicijalnu reprezentaciju u retku za svaku riječ koju učitate. Bitno: Pripazite da redoslijed vektorskih reprezentacija u matrici odgovara redoslijedu riječi u vokabularu! Npr., na indeksu 0 mora biti reprezentacija za posebni znak punjenja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b18e40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14806, 300])\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = prvi.get_embedding_matrix(text_vocab, 'data/sst_glove_6b_300d.txt', 300)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc7633",
   "metadata": {},
   "source": [
    "Jednom kad ste uspješno učitali vašu V×d embedding matricu, iskoristite torch.nn.Embedding.from_pretrained() kako bi vašu matricu spremili u optimizirani omotač za vektorske reprezentacije. Postavite parametar funkcije padding_idx na 0 (indeks znaka punjenja u vašoj embedding matrici), a parametar funkcije freeze ostavite na True ako koristite predtrenirane reprezentacije, a postavite na False inače."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6181411",
   "metadata": {},
   "source": [
    "## Nadjačavanje metoda torch.utils.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d248c2d",
   "metadata": {},
   "source": [
    "Da bi naša implementacija razreda NLPDataset bila potpuna, potrebno je nadjačati __getitem__ metodu koja omogućava indeksiranje razreda. Za potrebe vježbe, ta metoda treba vraćati numerikalizirani text i labelu referencirane instance. Također, dovoljno je napraviti da se numerikalizacija radi “on-the-fly”, i nije ju nužno cachirati.\n",
    "\n",
    "Primjer numerikalizacije s nadjačavanjem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94680c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['yet', 'the', 'act', 'is', 'still', 'charming', 'here']\n",
      "Label: positive\n",
      "Numericalized text: tensor([189,   2, 674,   7, 129, 348, 143])\n",
      "Numericalized label: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "instance_text, instance_label = train_dataset.instances[3]\n",
    "print(f\"Text: {instance_text}\")\n",
    "print(f\"Label: {instance_label}\")\n",
    "numericalized_text, numericalized_label = train_dataset[3]\n",
    "print(f\"Numericalized text: {numericalized_text}\")\n",
    "print(f\"Numericalized label: {numericalized_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87ca77",
   "metadata": {},
   "source": [
    "# Implementacija batchiranja podataka: collate funkcija"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70ef3a",
   "metadata": {},
   "source": [
    "Zadatak naše collate funkcije biti će nadopuniti duljine instanci znakom punjenja do duljine najdulje instance u batchu. Za ovo, pogledajte funkciju from torch.nn.utils.rnn.pad_sequence. Primjetite da vaša implementacija collate funkcije mora znati koji se indeks koristi kao znak punjenja.\n",
    "\n",
    "Jednom kad smo implementirali sve navedeno, naše učitavanje podataka bi moglo izgledati ovako:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5676d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: tensor([[   2,  554,    7, 2872,    6,   22,    2, 2873, 1236,    8,   96, 4800,\n",
      "            4,   10,   72,    8,  242,    6,   75,    3, 3576,   56, 3577,   34,\n",
      "         2022, 2874, 7123, 3578, 7124,   42,  779, 7125,    0,    0],\n",
      "        [   2, 2875, 2023, 4801,    5,    2, 3579,    5,    2, 2876, 4802,    7,\n",
      "           40,  829,   10,    3, 4803,    5,  627,   62,   27, 2877, 2024, 4804,\n",
      "          962,  715,    8, 7126,  555,    5, 7127, 4805,    8, 7128]])\n",
      "Labels: tensor([0, 0])\n",
      "Lengths: tensor([32, 34])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2 # Only for demonstrative purposes\n",
    "shuffle = False # Only for demonstrative purposes\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, \n",
    "                              shuffle=shuffle, collate_fn=prvi.pad_collate_fn)\n",
    "texts, labels, lengths = next(iter(train_data_loader))\n",
    "print(f\"Texts: {texts}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Lengths: {lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca866a4",
   "metadata": {},
   "source": [
    "# Zadatak 2. Implementacija baseline modela (25% bodova)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb6e91",
   "metadata": {},
   "source": [
    "Prvi korak kod svakog zadatka strojnog učenja bi trebao biti implementacija baseline modela. Baseline model nam služi za procjenu performansi koje naš stvarni, uobičajeno skuplji model mora moći preći kao plitak potok. Također, baseline modeli će nam pokazati kolika je stvarno cijena izvođenja naprednijih modela.\n",
    "\n",
    "Vaš zadatak u laboratorijskoj vježbi je implementirati model koji će koristiti sažimanje usrednjavanjem (eng. mean pooling) kako bi eliminirao problematičnu varijabilnu dimenziju. Pri primjeni sažimanja usrednjavanjem odmah eliminirajte cijelu vremensku dimenziju (tzv. okno je veličine T).\n",
    "\n",
    "Osnovni model koji implementirate mora izgledati ovako:\n",
    "\n",
    "    avg_pool() -> fc(300, 150) -> ReLU() -> fc(150, 150) -> ReLU() -> fc(150,1)\n",
    "    \n",
    "Kao gubitak predlažemo da koristite BCEWithLogitsLoss, u kojem slučaju ne morate primjeniti sigmoidu na izlaznim logitima. Alternativno, možete staviti da vam je izlazna dimenzionalnost broj klasa te koristiti gubitak unakrsne entropije. Oba pristupa su korištena u praksi ovisno o osobnim preferencama.\n",
    "\n",
    "Kao algoritam optimizacije koristite Adam.\n",
    "\n",
    "Implementirajte metrike praćenja performansi modela. Osim gubitka na skupu podataka, zanimaju nas preciznost (eng. accuracy), f1 mjera i matrica zabune (eng. confusion matrix). Nakon svake epohe ispišite performanse modela po svim metrikama na skupu za validaciju, a nakon zadnje epohe ispišite performanse modela na skupu za testiranje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75bbe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 results on validation dataset:\n",
      "Accuracy: 0.6578802855573861\n",
      "Precision: 0.6189300411522634\n",
      "F1 score: 0.7070992007522331\n",
      "Loss: 19.809267\n",
      "Confusion matrix:\n",
      " [[446 463]\n",
      " [160 752]]\n",
      "\n",
      "Epoch 2 results on validation dataset:\n",
      "Accuracy: 0.6749038989566173\n",
      "Precision: 0.6568627450980392\n",
      "F1 score: 0.6935817805383023\n",
      "Loss: 13.55767\n",
      "Confusion matrix:\n",
      " [[559 350]\n",
      " [242 670]]\n",
      "\n",
      "Epoch 3 results on validation dataset:\n",
      "Accuracy: 0.6891817682591982\n",
      "Precision: 0.6555755395683454\n",
      "F1 score: 0.7203557312252964\n",
      "Loss: 12.266848\n",
      "Confusion matrix:\n",
      " [[526 383]\n",
      " [183 729]]\n",
      "\n",
      "Epoch 4 results on validation dataset:\n",
      "Accuracy: 0.6743547501372872\n",
      "Precision: 0.627906976744186\n",
      "F1 score: 0.7253358036127837\n",
      "Loss: 12.924502\n",
      "Confusion matrix:\n",
      " [[445 464]\n",
      " [129 783]]\n",
      "\n",
      "Epoch 5 results on validation dataset:\n",
      "Accuracy: 0.6749038989566173\n",
      "Precision: 0.650093808630394\n",
      "F1 score: 0.7007077856420626\n",
      "Loss: 11.843177\n",
      "Confusion matrix:\n",
      " [[536 373]\n",
      " [219 693]]\n",
      "\n",
      "Results on test dataset:\n",
      "Accuracy: 0.6892201834862385\n",
      "Precision: 0.6611909650924025\n",
      "F1 score: 0.703825136612022\n",
      "Loss: 11.477589\n",
      "Confusion matrix:\n",
      " [[279 165]\n",
      " [106 322]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import drugi\n",
    "import numpy as np\n",
    "\n",
    "freq_data, freq_label = prvi.get_frequencies('data/sst_train_raw.csv')\n",
    "text_vocab = prvi.Vocab(freq_data, max_size=-1, min_freq=1)\n",
    "label_vocab = prvi.Vocab(freq_label, use_extra=False)\n",
    "train_dataset = prvi.NLPDataset('data/sst_train_raw.csv', text_vocab, label_vocab)\n",
    "embedding_matrix = prvi.get_embedding_matrix(text_vocab, 'data/sst_glove_6b_300d.txt', 300)\n",
    "\n",
    "seed=7052020\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = drugi.BaselineModel(embedding_matrix)\n",
    "model.train_model(epochs=5, dataset=train_dataset, optimizer=torch.optim.Adam(model.get_params(), lr=0.001), batch_size=10, text_vocab=text_vocab, label_vocab=label_vocab, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dbe663",
   "metadata": {},
   "source": [
    "# Zadatak 3. Implementacija povratne neuronske mreže (25% bodova)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea1489",
   "metadata": {},
   "source": [
    "Nakon što ste uspješno implementirali vaš baseline model, vrijeme je da isprobamo neki model baziran na povratnim neuronskim mrežama. Vaš zadatak je implementirati osnovni model povratne neuronske meže po izboru. Na izboru su vam iduće ćelije: [“Vanilla” RNN, GRU, LSTM].\n",
    "\n",
    "Za odabrani model, detaljno pročitajte njegovu dokumentaciju. U nastavku ćemo vam samo skrenuti pozornost na nekoliko bitnih detalja:\n",
    "\n",
    "- Svaka RNN mreža kao izlaz svoje forward metode vraća (1) niz skrivenih stanja posljednjeg sloja i (2) skriveno stanje (tj., skrivena stanja u slučaju LSTMa) za sve slojeve zadnjeg vremenskog koraka. Kao ulaz u dekoder obično želite staviti skriveno stanje iz zadnjeg sloja u zadnjem vremenskom koraku. Kod LSTMa, to je h komponenta dualnog (h, c) skrivenog stanja.\n",
    "\n",
    "- Radi brzine, RNN mreže preferiraju inpute u time-first formatu (budući da je brže iterirati po prvoj dimenziji tenzora). Transponirajte ulaze prije nego ih šaljete RNN ćeliji.\n",
    "\n",
    "- Tenzori koji su ulaz u RNN ćelije se često “pakiraju”. Pakiranje je zapis tenzora kojemu su pridružene stvarne duljine svakog elementa u batchu. Ako koristite pakiranje, RNN mreža se neće odmatati za vremenske korake koji sadrže padding u elementima batcha. Ovdje osim efikasnosti možete dobiti i na preciznosti, ali ovaj dio nije nužan dio vaše implementacije.\n",
    "Implementirajte gradient clipping prije optimizacijskog koraka \n",
    "\n",
    "- Osnovni model vaše odabrane RNN ćelije treba izgledati ovako:\n",
    "\n",
    "    rnn(150) -> rnn(150) -> fc(150, 150) -> ReLU() -> fc(150,1)\n",
    "    \n",
    "Vaš osnovni model RNN ćelije bi trebao biti jednosmjeran i imati dva sloja. Za višeslojni RNN iskoristite argument num_layers pri konstrukciji RNN mreže."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aeaf68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
